{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from time import time\n",
    "\n",
    "from qaoa.quantum_model import QuantumModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAOA_torch:\n",
    "    def __init__(self, hamiltonian, device=\"cpu\"):\n",
    "        self.hamiltonian = hamiltonian\n",
    "\n",
    "        self.n = hamiltonian.num_qubits\n",
    "        self.N = 2 ** self.n\n",
    "\n",
    "        self.driver = torch.from_numpy(self.hamiltonian.driver.to_matrix()).to(device)\n",
    "        self.mixer = torch.from_numpy(self.hamiltonian.mixer.to_matrix()).to(device)\n",
    "        self.target = torch.from_numpy(self.hamiltonian.target.to_matrix()).to(device)\n",
    "        \n",
    "    def energy(self, gammas, betas):\n",
    "        \n",
    "        assert gammas.shape == betas.shape\n",
    "        param_shape = gammas.shape\n",
    "\n",
    "        if len(param_shape) == 1:\n",
    "            batches = 1\n",
    "            p = param_shape[0]\n",
    "        elif len(gammas.shape) == 2:\n",
    "            batches, p = param_shape\n",
    "        else:\n",
    "            raise RuntimeError(\"gammas and betas must be 1 or 2 dimensional\")\n",
    "        \n",
    "        # (batches, N)\n",
    "        psi = torch.ones(batches, self.N, dtype=torch.complex128) / np.sqrt(self.N)\n",
    "\n",
    "        # (batches, p, 2)\n",
    "        parameters = torch.stack((gammas, betas)).permute(1, 2, 0)\n",
    "\n",
    "        # (2, N, N)\n",
    "        hamiltonians = torch.stack((self.driver, self.mixer))\n",
    "\n",
    "        # (batches, p, 2, N, N)\n",
    "        hamiltonians = hamiltonians.unsqueeze(0).unsqueeze(0).repeat(batches, p, 1, 1, 1)\n",
    "\n",
    "        hamiltonians = torch.einsum('bpc,bpcij->bpcij', parameters, hamiltonians)\n",
    "        # (batches * p * 2, N, N)\n",
    "        hamiltonians = hamiltonians.reshape(batches * p * 2, self.N, self.N)\n",
    "\n",
    "        hamiltonians = torch.matrix_exp(hamiltonians * -1j)\n",
    "\n",
    "        # (batches, p, 2, N, N)\n",
    "        hamiltonians = hamiltonians.view(batches, p, 2, self.N, self.N)\n",
    "        # (p, 2, batches, N, N)\n",
    "        hamiltonians = hamiltonians.permute(1, 2, 0, 3, 4)\n",
    "\n",
    "        for i in range(p):\n",
    "            batch_driver, batch_mixer = hamiltonians[i]\n",
    "            \n",
    "            psi = torch.einsum('bij,bj->bi', batch_driver, psi)\n",
    "            psi = torch.einsum('bij,bj->bi', batch_mixer, psi)\n",
    "        \n",
    "        del hamiltonians\n",
    "        gc.collect()\n",
    "\n",
    "        result = torch.einsum('ij,bj->bi', self.target, psi)\n",
    "        result = torch.einsum('bi,bi->b', psi.conj(), result)\n",
    "\n",
    "        return result.real\n",
    "\n",
    "    def energy_landscape(self, res, p, chunk_size = None):\n",
    "        s = torch.linspace(0, 2 * np.pi, res + 1)[:-1]\n",
    "        coords = torch.meshgrid(*[s] * (2 * p))\n",
    "        coords = torch.stack([coord.flatten() for coord in coords]).T\n",
    "        gammas, betas = torch.hsplit(coords, 2)\n",
    "\n",
    "        if chunk_size is None:\n",
    "            gb_used = 20\n",
    "            \n",
    "            psi_size = self.N\n",
    "            ham_size = p * 2 * self.N * self.N\n",
    "            coord_size = (2 * p) * (self.N ** (2 * p))\n",
    "\n",
    "            chunk_size = (gb_used / 4 * 1e9 - (16 * coord_size)) / (32 * (psi_size + ham_size))\n",
    "            chunk_size = max(chunk_size, 10)\n",
    "\n",
    "        num_chunks = int((res ** (2 * p)) / chunk_size)\n",
    "        num_chunks = max(num_chunks, 1)\n",
    "        \n",
    "        gamma_chunks = torch.chunk(gammas, num_chunks)\n",
    "        beta_chunks = torch.chunk(betas, num_chunks)\n",
    "\n",
    "        # clear massive memory used\n",
    "        del s\n",
    "        del coords\n",
    "        del gammas\n",
    "        del betas\n",
    "        gc.collect()\n",
    "\n",
    "        energies = []\n",
    "        for i in trange(num_chunks):\n",
    "            energies.append(self.energy(gamma_chunks[i], beta_chunks[i]))\n",
    "\n",
    "        energies = torch.cat(energies)\n",
    "        energies = energies.reshape([res] * (2 * p))\n",
    "        return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "driver:\n",
       "4.0 * IIII\n",
       "+ 2.0 * IIZZ\n",
       "+ 1.0 * IZIZ\n",
       "+ 1.0 * IZZI\n",
       "+ 4.0 * IIIZ\n",
       "- 1.0 * IIZI\n",
       "+ 3.0 * IZII\n",
       "- 2.0 * ZIII\n",
       "mixer:\n",
       "1.0 * IIIY\n",
       "+ 1.0 * IIYI\n",
       "+ 1.0 * IYII\n",
       "+ 1.0 * YIII\n",
       "target:\n",
       "1.0 * IIII\n",
       "+ 0.5 * IIZZ\n",
       "+ 0.25 * IZIZ\n",
       "+ 0.25 * IZZI\n",
       "+ 1.0 * IIIZ\n",
       "- 0.25 * IIZI\n",
       "+ 0.75 * IZII\n",
       "- 0.5 * ZIII\n",
       "+ 0.25 * IIXX\n",
       "+ 0.25 * IIYY\n",
       "+ 0.25 * IXXI\n",
       "+ 0.25 * IYYI\n",
       "+ 0.25 * XXII\n",
       "+ 0.25 * YYII"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Schwinger Model\n",
    "\"\"\"\n",
    "\n",
    "select = {'N': 4, #qubits\n",
    "          'g' : 1,  #coupling\n",
    "          'm' : 1,  #bare mass\n",
    "          'a' : 1, #lattice spacing\n",
    "          'theta' : 0, #topological term\n",
    "          'mixer_type' : 'Y', # type of mixer {'X', 'Y', 'XY'}\n",
    "         }\n",
    "\n",
    "model = QuantumModel('schwinger', 'Standard')\n",
    "\n",
    "\n",
    "ham = model.make(select)\n",
    "schwinger_target = ham.target\n",
    "\n",
    "torch_qaoa = QAOA_torch(ham)\n",
    "\n",
    "ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def repeated_tensor(base, exp):\n",
    "    result = base\n",
    "\n",
    "    if exp > 1:\n",
    "        result = repeated_tensor(torch.kron(base, base), exp//2)\n",
    "        if exp % 2 == 1:\n",
    "            result = torch.kron(result, base)\n",
    "\n",
    "    return result\n",
    "\n",
    "def coupling_eig(coupling_tensor):\n",
    "    eigvals = None\n",
    "\n",
    "    pauli_diags = [torch.tensor([1, 1], dtype=torch.complex128), torch.tensor([1, -1], dtype=torch.complex128)]\n",
    "    pauli_types = set()\n",
    "\n",
    "    for ndx in coupling_tensor.keys():\n",
    "        coeff = coupling_tensor[ndx]\n",
    "\n",
    "        term = None\n",
    "\n",
    "        for i in ndx:\n",
    "            pauli_types.add(i)\n",
    "            term = torch.kron(term, pauli_diags[min(i, 1)]) if term is not None else pauli_diags[min(i, 1)]\n",
    "        term = term * coeff\n",
    "        eigvals = eigvals + term if eigvals is not None else term\n",
    "\n",
    "    if (1 in pauli_types and 2 in pauli_types) or (2 in pauli_types and 3 in pauli_types) or (3 in pauli_types and 1 in pauli_types):\n",
    "        eigvals, eigvecs = torch.linalg.eigh(torch_qaoa.mixer)\n",
    "        return eigvals.type(torch.complex128), eigvecs\n",
    "\n",
    "    if 1 in pauli_types: # X\n",
    "        eigvecs = torch.tensor([[1, 1], [1, -1]], dtype=torch.complex128) / np.sqrt(2)\n",
    "    if 2 in pauli_types: # Y\n",
    "        eigvecs = torch.tensor([[1, 1], [1j, -1j]], dtype=torch.complex128) / np.sqrt(2)\n",
    "    else: # I and/or Z\n",
    "        eigvecs = torch.tensor([[1, 0], [0, 1]], dtype=torch.complex128)\n",
    "\n",
    "    eigvecs = repeated_tensor(eigvecs, coupling_tensor.n)\n",
    "\n",
    "    return eigvals, eigvecs\n",
    "\n",
    "eigvals, eigvecs = coupling_eig(ham.mixer_coupling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landscape_minimas(landscape, filter = True):\n",
    "\n",
    "    dims = len(landscape.shape)\n",
    "    res = landscape.shape[0]\n",
    "\n",
    "    minimas = torch.full(landscape.shape, True)\n",
    "\n",
    "    for dim in range(dims):\n",
    "        less_right = landscape <= torch.roll(landscape, 1 , dim)\n",
    "        less_left = landscape <= torch.roll(landscape, -1 , dim)\n",
    "\n",
    "        axis_minimas = torch.logical_and(less_right, less_left)\n",
    "        minimas = torch.logical_and(minimas, axis_minimas)\n",
    "\n",
    "    if filter:\n",
    "        global_minimas = torch.isclose(landscape, torch.min(landscape))\n",
    "        minimas = torch.logical_and(minimas, global_minimas)\n",
    "\n",
    "    min_energy = landscape[minimas]\n",
    "\n",
    "    points = torch.stack(torch.where(minimas)).T * (2*np.pi) / res\n",
    "    gammas, betas = torch.hsplit(points, 2)\n",
    "\n",
    "    return gammas, betas, min_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas, betas, min_energies = landscape_minimas(energy_scape)\n",
    "\n",
    "gammas = gammas.requires_grad_()\n",
    "betas = betas.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176: loss = -2.030924839764389, grad = 9.917445277096704e-05\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.SGD([gammas, betas], lr=1e-3)\n",
    "optimizer = torch.optim.Adam([gammas, betas], lr=1e-3)\n",
    "\n",
    "grad_mag = np.infty\n",
    "\n",
    "count = 0\n",
    "while grad_mag > 1e-4:\n",
    "    optimizer.zero_grad()\n",
    "    energy = torch_qaoa.energy(gammas, betas)\n",
    "    loss = torch.mean(energy)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    grad_mag = max(torch.max(gammas.grad.abs()), torch.max(betas.grad.abs()))\n",
    "\n",
    "    print(f\"{count}: loss = {loss}, grad = {grad_mag}\", end=\"\\t\\t\\r\")\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.0879, dtype=torch.float64, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269,\n",
       "        -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879,\n",
       "        -2.0000, -2.0000, -2.0865, -2.0864, -2.0779, -2.0777, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0710, -2.0710,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0865, -2.0864,\n",
       "        -2.0780, -2.0779, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0710, -2.0710, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0710, -2.0710, -2.0710, -2.0710, -2.0113, -2.0113, -2.0113, -2.0113,\n",
       "        -2.0372, -2.0372, -2.0372, -2.0372, -2.0111, -2.0111, -2.0111, -2.0111,\n",
       "        -2.0415, -2.0415, -2.0415, -2.0415, -2.0370, -2.0370, -2.0370, -2.0370,\n",
       "        -2.0405, -2.0405, -2.0405, -2.0405, -2.0053, -2.0053, -2.0053, -2.0053,\n",
       "        -2.0358, -2.0358, -2.0358, -2.0358, -2.0645, -2.0645, -2.0645, -2.0645,\n",
       "        -2.0000, -2.0000, -2.0863, -2.0861, -2.0778, -2.0776, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0710, -2.0710,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0862, -2.0861,\n",
       "        -2.0780, -2.0778, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0710, -2.0710, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0710, -2.0710, -2.0710, -2.0710, -2.0113, -2.0113, -2.0113, -2.0113,\n",
       "        -2.0372, -2.0372, -2.0372, -2.0372, -2.0111, -2.0111, -2.0111, -2.0111,\n",
       "        -2.0415, -2.0415, -2.0415, -2.0415, -2.0370, -2.0370, -2.0370, -2.0370,\n",
       "        -2.0405, -2.0405, -2.0405, -2.0405, -2.0053, -2.0053, -2.0053, -2.0053,\n",
       "        -2.0358, -2.0358, -2.0358, -2.0358, -2.0645, -2.0645, -2.0645, -2.0645,\n",
       "        -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051,\n",
       "        -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269,\n",
       "        -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879,\n",
       "        -2.0000, -2.0000, -2.0864, -2.0862, -2.0776, -2.0775, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0710, -2.0710,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0864, -2.0862,\n",
       "        -2.0778, -2.0777, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0710, -2.0710, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0710, -2.0710, -2.0710, -2.0710, -2.0113, -2.0113, -2.0113, -2.0113,\n",
       "        -2.0372, -2.0372, -2.0372, -2.0372, -2.0111, -2.0111, -2.0111, -2.0111,\n",
       "        -2.0415, -2.0415, -2.0415, -2.0415, -2.0370, -2.0370, -2.0370, -2.0370,\n",
       "        -2.0405, -2.0405, -2.0405, -2.0405, -2.0053, -2.0053, -2.0053, -2.0053,\n",
       "        -2.0358, -2.0358, -2.0358, -2.0358, -2.0645, -2.0645, -2.0645, -2.0645,\n",
       "        -2.0000, -2.0000, -2.0864, -2.0862, -2.0779, -2.0778, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0710, -2.0710,\n",
       "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0864, -2.0862,\n",
       "        -2.0781, -2.0780, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0000, -2.0000, -2.0710, -2.0710, -2.0000, -2.0000, -2.0000, -2.0000,\n",
       "        -2.0710, -2.0710, -2.0710, -2.0710, -2.0113, -2.0113, -2.0113, -2.0113,\n",
       "        -2.0372, -2.0372, -2.0372, -2.0372, -2.0111, -2.0111, -2.0111, -2.0111,\n",
       "        -2.0415, -2.0415, -2.0415, -2.0415, -2.0370, -2.0370, -2.0370, -2.0370,\n",
       "        -2.0405, -2.0405, -2.0405, -2.0405, -2.0053, -2.0053, -2.0053, -2.0053,\n",
       "        -2.0358, -2.0358, -2.0358, -2.0358, -2.0645, -2.0645, -2.0645, -2.0645,\n",
       "        -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879, -2.0879,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399, -2.0399,\n",
       "        -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051, -2.0051,\n",
       "        -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269, -2.0269],\n",
       "       dtype=torch.float64, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_qaoa.energy(gammas, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(torch.isclose(energy, torch.min(energy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 100\n",
    "p = 3\n",
    "\n",
    "gammas = torch.rand(batches, p) * 2 * np.pi\n",
    "betas = torch.rand(batches, p) * 2 * np.pi\n",
    "\n",
    "gammas = gammas.requires_grad_()\n",
    "betas = betas.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1807.0633333333333\n"
     ]
    }
   ],
   "source": [
    "tenant_water_3 = 153.99 + 192.78 + 179.85 + 192.78 + 166.92 + 208.03\n",
    "tenant_water_2 = 201.20 + 160.59 + 187.67 + 186.79 + 158.75\n",
    "\n",
    "tenant_electric_3 = 65.86 + 80.13 + 59.04 + 88.81 + 109.01 + 108.77 + 152.47 + 86.38 + 6.3 + 82.32 + 110.52 + 104.35\n",
    "tenant_electric_2 =  107.28 + 80.5 + 35.55 + 109.93 + 185.87 + 205.26 + 232.78 + 138.36 + 116.95 + 74.44\n",
    "\n",
    "total = (tenant_water_3 / 3) + (tenant_water_2 / 2) + (tenant_electric_3 / 3) + (tenant_electric_2 / 2)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0, 1], [1, 0]], dtype=torch.complex128)\n",
    "Y = torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex128)\n",
    "Z = torch.tensor([[1, 0], [0, -1]], dtype=torch.complex128)\n",
    "I = torch.tensor([[1, 0], [0, 1]], dtype=torch.complex128)\n",
    "\n",
    "X_eigvecs = torch.tensor([[1, 1], [1, -1]], dtype=torch.complex128)\n",
    "Y_eigvecs = torch.tensor([[1, 1], [1j, -1j]], dtype=torch.complex128)\n",
    "Z_eigvecs = torch.tensor([[1, 0], [0, 1]], dtype=torch.complex128)\n",
    "\n",
    "theta = torch.Tensor([1])\n",
    "# for i in range(1_200_000):\n",
    "#     torch.cos(theta) * torch.eye(2) - 1j * torch.sin(theta) * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2371e-17+0.j, 1.0000e+00+0.j],\n",
       "        [1.0000e+00+0.j, 2.2371e-17+0.j]], dtype=torch.complex128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals, eigvecs = torch.linalg.eigh(X)  \n",
    "\n",
    "eigvecs @ torch.diag(eigvals).type(torch.complex128) @ eigvecs.conj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.+0.j,  1.+0.j],\n",
       "        [ 1.+0.j,  1.+0.j]], dtype=torch.complex128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvecs * np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def repeated_tensor(base, exp):\n",
    "    result = base\n",
    "\n",
    "    if exp > 1:\n",
    "        result = repeated_tensor(torch.kron(base, base), exp//2)\n",
    "        if exp % 2 == 1:\n",
    "            result = torch.kron(result, base)\n",
    "\n",
    "    return result\n",
    "    \n",
    "repeated_tensor(X, 12).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.+0.j, -2.+0.j,  8.+0.j,  2.+0.j,  2.+0.j, -8.+0.j,  2.+0.j,  0.+0.j, 16.+0.j,  2.+0.j,\n",
       "        12.+0.j,  6.+0.j,  6.+0.j, -4.+0.j,  6.+0.j,  4.+0.j],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(ham.driver.to_matrix()).diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PauliSumOp(SparsePauliOp(['IIII', 'IIZZ', 'IZIZ', 'IZZI', 'IIIZ', 'IIZI', 'IZII', 'ZIII'],\n",
       "              coeffs=[ 4.+0.j,  2.+0.j,  1.+0.j,  1.+0.j,  4.+0.j, -1.+0.j,  3.+0.j, -2.+0.j]), coeff=1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham.driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.+0.j, -1.+0.j], dtype=torch.complex128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_tensor(Z, 1).diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 000\n",
    "# 001\n",
    "# 010\n",
    "# 011\n",
    "# 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j, -1.+0.j, -1.+0.j, -1.+0.j, -1.+0.j],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.kron(Z, torch.kron(I, I)).diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 1, 0, 1, 0, 1])\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 1])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "size = 8\n",
    "print(torch.arange(size) & 1)\n",
    "print((torch.arange(size) >> 1) & 1)\n",
    "print((torch.arange(size) >> 2) & 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j],\n",
       "        [ 0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j],\n",
       "        [ 0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j],\n",
       "        [ 4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j],\n",
       "        [ 0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j],\n",
       "        [ 4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j],\n",
       "        [ 4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j],\n",
       "        [ 0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j],\n",
       "        [ 0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j],\n",
       "        [ 4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j],\n",
       "        [ 4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j],\n",
       "        [ 0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j],\n",
       "        [ 4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j,\n",
       "          0.0000e+00-1.0000e+00j,  4.9304e-32+0.0000e+00j],\n",
       "        [ 0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+4.9304e-32j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j],\n",
       "        [ 0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+4.9304e-32j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j,\n",
       "         -4.9304e-32+0.0000e+00j,  0.0000e+00-1.0000e+00j],\n",
       "        [-4.9304e-32+0.0000e+00j,  0.0000e+00-4.9304e-32j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00-4.9304e-32j,  4.9304e-32+0.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          4.9304e-32+0.0000e+00j,  0.0000e+00+1.0000e+00j,\n",
       "          0.0000e+00+1.0000e+00j, -4.9304e-32+0.0000e+00j]],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.+0.j, 0.-1.j, 0.-1.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.+0.j,\n",
       "        0.-1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j,\n",
       "        0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j,\n",
       "        0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+1.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j,\n",
       "        0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.-1.j, 0.+0.j,\n",
       "        0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j,\n",
       "        0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j,\n",
       "        0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+1.j, 0.+1.j, 0.+0.j,\n",
       "        0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j],\n",
       "       [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n",
       "        0.+0.j, 0.-1.j, 0.-1.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n",
       "        0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n",
       "        0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n",
       "        0.+0.j, 0.+1.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j,\n",
       "        0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.-1.j, 0.-1.j, 0.+0.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j,\n",
       "        0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j,\n",
       "        0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.-1.j],\n",
       "       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j,\n",
       "        0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+1.j, 0.+1.j, 0.+0.j]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham.mixer.to_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
