{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "from qaoa.quantum_model import QuantumModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "def save(filename, data):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load(filename,):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "@lru_cache\n",
    "def repeated_tensor(base, exp):\n",
    "    result = base\n",
    "\n",
    "    if exp > 1:\n",
    "        result = repeated_tensor(torch.kron(base, base), exp//2)\n",
    "        if exp % 2 == 1:\n",
    "            result = torch.kron(result, base)\n",
    "\n",
    "    return result\n",
    "\n",
    "def coupling_eig(coupling_tensor):\n",
    "    eigvals = None\n",
    "\n",
    "    pauli_diags = [torch.tensor([1, 1], dtype=torch.complex128), torch.tensor([1, -1], dtype=torch.complex128)]\n",
    "    pauli_types = set()\n",
    "\n",
    "    for ndx in coupling_tensor.keys():\n",
    "        coeff = coupling_tensor[ndx]\n",
    "\n",
    "        term = None\n",
    "\n",
    "        for i in ndx:\n",
    "            pauli_types.add(i)\n",
    "            term = torch.kron(term, pauli_diags[min(i, 1)]) if term is not None else pauli_diags[min(i, 1)]\n",
    "        term = term * coeff\n",
    "        eigvals = eigvals + term if eigvals is not None else term\n",
    "\n",
    "    if (1 in pauli_types and 2 in pauli_types) or (2 in pauli_types and 3 in pauli_types) or (3 in pauli_types and 1 in pauli_types):\n",
    "        raise NotImplementedError(\"NotImplemented for more than one pauli type\")\n",
    "\n",
    "    if 1 in pauli_types: # X\n",
    "        eigvecs = torch.tensor([[1, 1], [1, -1]], dtype=torch.complex128) / np.sqrt(2)\n",
    "    if 2 in pauli_types: # Y\n",
    "        eigvecs = torch.tensor([[1, 1], [1j, -1j]], dtype=torch.complex128) / np.sqrt(2)\n",
    "    else: # I and/or Z\n",
    "        eigvecs = torch.tensor([[1, 0], [0, 1]], dtype=torch.complex128)\n",
    "\n",
    "    eigvecs = repeated_tensor(eigvecs, coupling_tensor.n)\n",
    "\n",
    "    return eigvals, eigvecs\n",
    "\n",
    "def parameter_eigs_exp(parameters, eigvals, eigvecs):\n",
    "\n",
    "    N = len(eigvals)\n",
    "\n",
    "    output_shape = parameters.shape + torch.Size([N, N])\n",
    "\n",
    "    parameters = parameters.flatten()\n",
    "    \n",
    "    # (p), (N) -> (p, N)\n",
    "    eig_exps = torch.exp(torch.einsum('p,N->pN', parameters, eigvals))\n",
    "\n",
    "    # recombine (p, N), (N, N) -> (p, N, N)\n",
    "    eig_exps = torch.einsum('pi,ij->pij', eig_exps, eigvecs.adjoint())\n",
    "    eig_exps = torch.einsum('ij,pjk->pik', eigvecs, eig_exps)\n",
    "\n",
    "    return eig_exps.reshape(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAOA_torch:\n",
    "    def __init__(self, hamiltonian, device=\"cpu\"):\n",
    "        self.hamiltonian = hamiltonian\n",
    "\n",
    "        self.n = hamiltonian.num_qubits\n",
    "        self.N = 2 ** self.n\n",
    "\n",
    "        self.driver_coupling = self.hamiltonian.driver_coupling\n",
    "        self.mixer_coupling = self.hamiltonian.mixer_coupling\n",
    "\n",
    "        self.driver = torch.from_numpy(self.hamiltonian.driver.to_matrix()).to(device)\n",
    "        self.mixer = torch.from_numpy(self.hamiltonian.mixer.to_matrix()).to(device)\n",
    "        self.target = torch.from_numpy(self.hamiltonian.target.to_matrix()).to(device)\n",
    "        \n",
    "    def energy(self, gammas, betas):\n",
    "        \n",
    "        assert gammas.shape == betas.shape\n",
    "        param_shape = gammas.shape\n",
    "\n",
    "        batches, p = param_shape\n",
    "        \n",
    "        # (batches, N)\n",
    "        psi = torch.ones(batches, self.N, dtype=torch.complex128) / np.sqrt(self.N)\n",
    "\n",
    "        driver_eigvals, driver_eigvecs = coupling_eig(self.driver_coupling)\n",
    "        mixer_eigvals, mixer_eigvecs = coupling_eig(self.mixer_coupling)\n",
    "\n",
    "        exp_driver = parameter_eigs_exp(gammas.T * -1j, driver_eigvals, driver_eigvecs)\n",
    "        exp_mixer = parameter_eigs_exp(betas.T * -1j, mixer_eigvals, mixer_eigvecs)\n",
    "        \n",
    "        for i in range(p):\n",
    "            psi = torch.einsum('bij,bj->bi', exp_driver[i], psi)\n",
    "            psi = torch.einsum('bij,bj->bi', exp_mixer[i], psi)\n",
    "        \n",
    "        del exp_driver\n",
    "        del exp_mixer\n",
    "        gc.collect()\n",
    "\n",
    "        result = torch.einsum('ij,bj->bi', self.target, psi)\n",
    "        result = torch.einsum('bi,bi->b', psi.conj(), result)\n",
    "\n",
    "        return result.real\n",
    "    \n",
    "    def estimate_chunk_size(self, p):\n",
    "        gb_used = 20\n",
    "        \n",
    "        psi_size = self.N\n",
    "        ham_size = p * 2 * self.N * self.N\n",
    "        coord_size = (2 * p) * (self.N ** (2 * p))\n",
    "\n",
    "        chunk_size = (gb_used/3 * 1e9 - (16 * coord_size)) / (32 * (psi_size + ham_size))\n",
    "\n",
    "        return max(chunk_size, 10)\n",
    "\n",
    "    def energy_landscape(self, res, p, chunk_size = None):\n",
    "        s = torch.linspace(0, 2 * np.pi, res + 1)[:-1]\n",
    "        coords = torch.meshgrid(*[s] * (2 * p))\n",
    "        coords = torch.stack([coord.flatten() for coord in coords]).T\n",
    "        gammas, betas = torch.hsplit(coords, 2)\n",
    "\n",
    "        if chunk_size is None:\n",
    "            chunk_size = self.estimate_chunk_size(p)\n",
    "\n",
    "        num_chunks = int(max((res ** (2 * p)) / chunk_size, 1))\n",
    "        \n",
    "        gamma_chunks = torch.chunk(gammas, num_chunks)\n",
    "        beta_chunks = torch.chunk(betas, num_chunks)\n",
    "\n",
    "        # clear massive memory used\n",
    "        del s\n",
    "        del coords\n",
    "        del gammas\n",
    "        del betas\n",
    "        gc.collect()\n",
    "\n",
    "        energies = []\n",
    "        for i in trange(num_chunks):\n",
    "            energies.append(self.energy(gamma_chunks[i], beta_chunks[i]))\n",
    "\n",
    "        energies = torch.cat(energies)\n",
    "        energies = energies.reshape([res] * (2 * p))\n",
    "        return energies\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "driver:\n",
       "4.0 * IIII\n",
       "+ 2.0 * IIZZ\n",
       "+ 1.0 * IZIZ\n",
       "+ 1.0 * IZZI\n",
       "+ 4.0 * IIIZ\n",
       "- 1.0 * IIZI\n",
       "+ 3.0 * IZII\n",
       "- 2.0 * ZIII\n",
       "mixer:\n",
       "1.0 * IIIY\n",
       "+ 1.0 * IIYI\n",
       "+ 1.0 * IYII\n",
       "+ 1.0 * YIII\n",
       "target:\n",
       "1.0 * IIII\n",
       "+ 0.5 * IIZZ\n",
       "+ 0.25 * IZIZ\n",
       "+ 0.25 * IZZI\n",
       "+ 1.0 * IIIZ\n",
       "- 0.25 * IIZI\n",
       "+ 0.75 * IZII\n",
       "- 0.5 * ZIII\n",
       "+ 0.25 * IIXX\n",
       "+ 0.25 * IIYY\n",
       "+ 0.25 * IXXI\n",
       "+ 0.25 * IYYI\n",
       "+ 0.25 * XXII\n",
       "+ 0.25 * YYII"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Schwinger Model\n",
    "\"\"\"\n",
    "\n",
    "select = {'N': 4, #qubits\n",
    "          'g' : 1,  #coupling\n",
    "          'm' : 1,  #bare mass\n",
    "          'a' : 1, #lattice spacing\n",
    "          'theta' : 0, #topological term\n",
    "          'mixer_type' : 'Y', # type of mixer {'X', 'Y', 'XY'}\n",
    "         }\n",
    "\n",
    "model = QuantumModel('schwinger', 'Standard')\n",
    "\n",
    "\n",
    "ham = model.make(select)\n",
    "schwinger_target = ham.target\n",
    "\n",
    "torch_qaoa = QAOA_torch(ham)\n",
    "\n",
    "ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.11it/s]\n"
     ]
    }
   ],
   "source": [
    "p = 1\n",
    "res = 32\n",
    "torch_qaoa = QAOA_torch(ham)\n",
    "energy_scape = torch_qaoa.energy_landscape(res, p)\n",
    "save(f\"data/{p}p_schwinger_{res}\", energy_scape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7500, 1.4039, 1.1679,  ..., 2.8873, 2.5821, 2.1693],\n",
       "        [0.8227, 1.0090, 1.1266,  ..., 0.4866, 0.5095, 0.6336],\n",
       "        [0.5581, 0.6336, 0.6831,  ..., 0.2506, 0.3447, 0.4566],\n",
       "        ...,\n",
       "        [1.2648, 1.1927, 1.1858,  ..., 1.7230, 1.5795, 1.4059],\n",
       "        [0.5581, 0.6336, 0.6831,  ..., 0.2506, 0.3447, 0.4566],\n",
       "        [0.8227, 1.0090, 1.1266,  ..., 0.4866, 0.5095, 0.6336]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_scape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landscape_minimas(landscape, filter = True):\n",
    "\n",
    "    dims = len(landscape.shape)\n",
    "    res = landscape.shape[0]\n",
    "\n",
    "    minimas = torch.full(landscape.shape, True)\n",
    "\n",
    "    for dim in range(dims):\n",
    "        less_right = landscape <= torch.roll(landscape, 1 , dim)\n",
    "        less_left = landscape <= torch.roll(landscape, -1 , dim)\n",
    "\n",
    "        axis_minimas = torch.logical_and(less_right, less_left)\n",
    "        minimas = torch.logical_and(minimas, axis_minimas)\n",
    "\n",
    "    if filter:\n",
    "        global_minimas = torch.isclose(landscape, torch.min(landscape))\n",
    "        minimas = torch.logical_and(minimas, global_minimas)\n",
    "\n",
    "    min_energy = landscape[minimas]\n",
    "\n",
    "    points = torch.stack(torch.where(minimas)).T * (2*np.pi) / res\n",
    "    gammas, betas = torch.hsplit(points, 2)\n",
    "\n",
    "    return gammas, betas, min_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 sample points\n"
     ]
    }
   ],
   "source": [
    "gammas, betas, min_energies = landscape_minimas(energy_scape)\n",
    "\n",
    "gammas = gammas.requires_grad_()\n",
    "betas = betas.requires_grad_()\n",
    "\n",
    "print(f\"{len(min_energies)} sample points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198: loss = -2.094262845442251, grad = 9.777047671377659e-05\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([gammas, betas], lr=1e-3)\n",
    "\n",
    "grad_mag = np.infty\n",
    "\n",
    "count = 0\n",
    "while grad_mag > 1e-4 and count < 10_000:\n",
    "    optimizer.zero_grad()\n",
    "    energy = torch_qaoa.energy(gammas, betas)\n",
    "    loss = torch.mean(energy)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    grad_mag = max(torch.max(gammas.grad.abs()), torch.max(betas.grad.abs()))\n",
    "\n",
    "    print(f\"{count}: loss = {loss}, grad = {grad_mag}\", end=\"\\t\\t\\r\")\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.0947, dtype=torch.float64, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947,\n",
       "        -2.0938, -2.0938, -2.0938, -2.0938, -2.0947, -2.0947, -2.0947, -2.0947],\n",
       "       dtype=torch.float64, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_qaoa.energy(gammas, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(torch.isclose(energy, torch.min(energy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 100\n",
    "p = 3\n",
    "\n",
    "gammas = torch.rand(batches, p) * 2 * np.pi\n",
    "betas = torch.rand(batches, p) * 2 * np.pi\n",
    "\n",
    "gammas = gammas.requires_grad_()\n",
    "betas = betas.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0, 1], [1, 0]], dtype=torch.complex128)\n",
    "Y = torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex128)\n",
    "Z = torch.tensor([[1, 0], [0, -1]], dtype=torch.complex128)\n",
    "I = torch.tensor([[1, 0], [0, 1]], dtype=torch.complex128)\n",
    "\n",
    "X_eigvecs = torch.tensor([[1, 1], [1, -1]], dtype=torch.complex128) / np.sqrt(2)\n",
    "Y_eigvecs = torch.tensor([[1, 1], [1j, -1j]], dtype=torch.complex128) / np.sqrt(2)\n",
    "Z_eigvecs = torch.tensor([[1, 0], [0, 1]], dtype=torch.complex128) / np.sqrt(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1.,  1.,  1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals, eigvecs = torch.linalg.eigh(torch.kron(X, I))\n",
    "\n",
    "eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7071-0.j, -0.0000-0.j,  0.0000+0.j, -0.7071+0.j],\n",
       "        [ 0.0000+0.j,  0.7071+0.j,  0.7071+0.j,  0.0000+0.j],\n",
       "        [ 0.7071+0.j,  0.0000+0.j,  0.0000+0.j, -0.7071+0.j],\n",
       "        [-0.0000-0.j, -0.7071-0.j,  0.7071+0.j,  0.0000+0.j]],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000+0.j,  0.5000+0.j,  0.5000+0.j,  0.5000+0.j],\n",
       "        [ 0.5000+0.j, -0.5000+0.j,  0.5000+0.j, -0.5000+0.j],\n",
       "        [ 0.5000+0.j,  0.5000+0.j, -0.5000+0.j, -0.5000+0.j],\n",
       "        [ 0.5000+0.j, -0.5000+0.j, -0.5000+0.j,  0.5000-0.j]],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.kron(X_eigvecs, X_eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_QAOA(gammas, betas, driver_coupling, mixer_coupling):\n",
    "    driver_eigvals, driver_eigvecs = coupling_eig(driver_coupling)\n",
    "    mixer_eigvals, mixer_eigvecs = coupling_eig(mixer_coupling)\n",
    "\n",
    "    param_shape = gammas.shape\n",
    "    if len(param_shape) == 1:\n",
    "        batches = 1\n",
    "        p = param_shape[0]\n",
    "    elif len(gammas.shape) == 2:\n",
    "        batches, p = param_shape\n",
    "    else:\n",
    "        raise RuntimeError(\"gammas and betas must be 1 or 2 dimensional\")\n",
    "\n",
    "    # (2, batches, p)\n",
    "    parameters = torch.stack((gammas, betas))\n",
    "\n",
    "    # (2, N)\n",
    "    eigvals = torch.stack([driver_eigvals, mixer_eigvals])\n",
    "\n",
    "    # (2, N, N)\n",
    "    eigvecs = torch.stack([driver_eigvecs, mixer_eigvecs])\n",
    "    adjoint_eigvecs = torch.stack([driver_eigvecs.adjoint(), mixer_eigvecs.adjoint()])\n",
    "\n",
    "    # (batches, p, 2, N) -> (2, batches, p, N)\n",
    "    eigvals = eigvals.unsqueeze(0).unsqueeze(0).repeat(batches, p, 1, 1).permute(2, 0, 1, 3)\n",
    "\n",
    "    # multiply by coefficients (2, batches, p), (2, batches, p, N) -> (2, batches, p, N)\n",
    "    eigvals = torch.einsum('cbp,cbpn->cbpn', parameters * -1j, eigvals)\n",
    "\n",
    "    # exponentiate\n",
    "    eigvals = torch.exp(eigvals)\n",
    "\n",
    "    # recombine (2, batches, p, N), (2, N, N) -> (2, batches, p, N, N)\n",
    "    eigvals = torch.einsum('cbpi,cij->cbpij', eigvals, adjoint_eigvecs)\n",
    "\n",
    "    # recombine (2, N, N), (2, batches, p, N, N) -> (2, batches, p, N, N) -> (p, 2, batches, N, N)\n",
    "    eigvals = torch.einsum('cij, cbpjk->cbpik', eigvecs, eigvals)\n",
    "\n",
    "    return eigvals.permute(2, 0, 1, 3, 4)\n",
    "\n",
    "def parameter_eigs_exp(parameters, eigvals, eigvecs):\n",
    "\n",
    "    N = len(eigvals)\n",
    "\n",
    "    output_shape = parameters.shape + torch.Size([N, N])\n",
    "\n",
    "    parameters = parameters.flatten()\n",
    "    \n",
    "    # (p), (N) -> (p, N)\n",
    "    eig_exps = torch.exp(torch.einsum('p,N->pN', parameters, eigvals))\n",
    "\n",
    "    # recombine (p, N), (N, N) -> (p, N, N)\n",
    "    eig_exps = torch.einsum('pi,ij->pij', eig_exps, eigvecs.adjoint())\n",
    "    eig_exps = torch.einsum('ij,pjk->pik', eigvecs, eig_exps)\n",
    "\n",
    "    return eig_exps.reshape(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 100\n",
    "p = 1\n",
    "s = torch.linspace(0, 2 * np.pi, res + 1)[:-1]\n",
    "coords = torch.meshgrid(*[s] * (2 * p))\n",
    "coords = torch.stack([coord.flatten() for coord in coords]).T\n",
    "gammas, betas = torch.hsplit(coords, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 10000, 16, 16])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scape = exp_QAOA(gammas, betas, torch_qaoa.driver_coupling, torch_qaoa.mixer_coupling)\n",
    "scape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_eigvals, driver_eigvecs = coupling_eig(torch_qaoa.driver_coupling)\n",
    "mixer_eigvals, mixer_eigvecs = coupling_eig(torch_qaoa.mixer_coupling)\n",
    "\n",
    "scape_driver_ = parameter_eigs_exp(gammas * -1j, driver_eigvals, driver_eigvecs)\n",
    "scape_mixer_ = parameter_eigs_exp(betas * -1j, mixer_eigvals, mixer_eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 10000, 16, 16])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scape_ = torch.stack([scape_driver_, scape_mixer_], 1)\n",
    "scape_ = scape_.permute(2, 1, 0, 3, 4)\n",
    "scape_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]]],\n",
       "\n",
       "\n",
       "         [[[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "          [[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           ...,\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "           [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]]]]],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scape - scape_) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
